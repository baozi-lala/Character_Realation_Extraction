# model
dataset: PRE_data

global_rep: true
local_rep: false
context_att: true

query: global # global

pretrain_l_m:  none # bert-base-chinese,bert-base,chinese, none,xlnet-base, xlnet-large
lstm_encoder: false
more_lstm: false

# encoder
emb_method: False # true for load_embeds
word_dim: 768
lstm_dim: 256
out_dim: 256
#type_dim: 20
#dist_dim: 20
finaldist: false
types: false
bilstm_layers: 2
rgcn_hidden_dim: 256
rgcn_num_layers: 2
gcn_in_drop: 0.2
gcn_out_drop: 0.2

# network
batch: 1
epoch: 200
drop_i: 0.5 # 0.5
drop_m: 0.0
drop_o: 0.3 # 0.3
att_head_num: 2
att_dropout: 0.0
lr: 0.0005
bert_lr: 0.00001
gc: 10
reg: 0.000
opt: adam
patience: 10
unk_w_prob: 0.5
min_w_freq: 1
init_train_epochs: 10
NA_NUM: 0.5  # 0.1==5:1
mlp_layers: 1
mlp_dim: 512

# gru
sen_len : 70  # 60：每个句子的固定长度（词个数）：如果真实句子长度大于该值，则舍弃后面的，小于则补充
#  pre_embed : flags.pre_embed  # 是否进行了训练（得到word2vec的词向量）
add_pos : true
pos_limit : 15  # 词与实体最大的距离
pos_dim : 5  # 设置位置嵌入的维度
#  hidden_dim : flags.hidden_dim
#  maxWordLen: 256 #与Bert对齐，不能改变
input_gru: 256
gru_dim: 256
output_gru: 256
gru_layers: 2
gru_dropout: 0.5  # 失活概率：0.5
pretrained_model: 'Bert'

# data based
train_data: ../data/DocPRE/processed/train1_v2.json
test_data: ../data/DocPRE/processed/dev1_v2.json
embeds: ../data/DocPRE/word_embed.txt
folder: ../results/docpre-dev
save_pred: dev

# options (chosen from parse input otherwise false)
lowercase: false
plot: true
show_class: false
early_stop: true
save_model: true
freeze_words: true

more_gru: true
# extra
seed: 0
shuffle_data: true
label2ignore: unknown
primary_metric: micro_f
#direction: l2r+r2l
gpu: 1